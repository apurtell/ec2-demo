# Make sure some important settings are in effect

ulimit -n 65536
ulimit -u 65536
sysctl -w fs.file-max=131072
sysctl -w vm.swappiness=0

echo "Entering setup-remote file"
HOSTNAME=`hostname --fqdn | awk '{print tolower($1)}'`
HOST_IP=$(host $HOSTNAME | awk '{print $4}')
export USER="root"
echo "$HOST_IP $HOSTNAME" >> /etc/hosts

# Configure Kerberos

if [ "$IS_MASTER" = "true" ]; then

####################
# KDC.CONF
#

  cat > /var/kerberos/krb5kdc/kdc.conf <<EOF
[kdcdefaults]
  v4_mode = nopreauth
  kdc_ports = 0
  kdc_tcp_ports = 88
[realms]
  HADOOP.LOCALDOMAIN = {
    master_key_type = des3-hmac-sha1
    acl_file = /var/kerberos/krb5kdc/kadm5.acl
    dict_file = /usr/share/dict/words
    admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
    supported_enctypes = des-cbc-md5:normal des-cbc-crc:normal des:normal des:v4 des:norealm des:onlyrealm
    max_life = 1d 0h 0m 0s
    max_renewable_life = 7d 0h 0m 0s
  }
EOF

####################
# KADM5 ACL

  cat > /var/kerberos/krb5kdc/kadm5.acl <<EOF
*/admin@HADOOP.LOCALDOMAIN    *
EOF

fi

####################
# KRB5 CONF

cat > /etc/krb5.conf <<EOF
[logging]
  default = FILE:/var/log/krb5libs.log
  kdc = FILE:/var/log/krb5kdc.log
  admin_server = FILE:/var/log/kadmind.log
[libdefaults]
  default_realm = HADOOP.LOCALDOMAIN
  dns_lookup_realm = false
  dns_lookup_kdc = false
  ticket_lifetime = 1d
  renew_lifetime = 7d
  forwardable = yes
  proxiable = yes
  udp_preference_limit = 1
  extra_addresses = 127.0.0.1
  kdc_timesync = 1
  ccache_type = 4
  allow_weak_crypto = true
[realms]
  HADOOP.LOCALDOMAIN = {
    kdc = ${MASTER_HOST}:88
    admin_server = ${MASTER_HOST}:749
  }
[domain_realm]
  localhost = HADOOP.LOCALDOMAIN
  .compute-1.internal = HADOOP.LOCALDOMAIN
  .internal = HADOOP.LOCALDOMAIN
  internal = HADOOP.LOCALDOMAIN
[appdefaults]
  pam = {
    debug = false
    ticket_lifetime = 36000
    renew_lifetime = 36000
    forwardable = true
    krb4_convert = false
  }
[login]
  krb4_convert = true
  krb4_get_tickets = false
EOF

    kdc_setup() {
      kmasterpass=$1
      kadmpass=$2
      kdb5_util create -s -P ${kmasterpass}
      service krb5kdc start
      service kadmin start
      kadmin.local <<EOF 
        add_principal -pw $kadmpass kadmin/admin
        add_principal -pw $kadmpass hadoop/admin
        add_principal -pw $kadmpass hdfs
        add_principal -pw $kadmpass mapred
        add_principal -pw $kadmpass hbase
        quit
EOF
}

    add_client() {
      user=$1
      pass=$2
      kt=$3
      host=$4
      kadmin -p $user -w $pass <<EOF 
        add_principal -randkey host/$host
        add_principal -randkey hadoop/$host
        add_principal -randkey hdfs/$host
        add_principal -randkey mapred/$host
        add_principal -randkey hbase/$host
        add_principal -randkey zookeeper/$host
        ktadd -k $kt host/$host
        ktadd -k $kt hadoop/$host
        ktadd -k $kt hdfs/$host
        ktadd -k $kt mapred/$host
        ktadd -k $kt hbase/$host
        ktadd -k $kt zookeeper/$host
        quit
EOF
     }

KDC_MASTER_PASS="EiSei0Da"
KDC_ADMIN_PASS="Chohpet6"
if [ "$IS_MASTER" = "true" ]; then
  kdc_setup $KDC_MASTER_PASS $KDC_ADMIN_PASS
  echo "Waiting 10 seconds for KDC"
  sleep 10
fi
mkdir -p /etc/hadoop/conf /etc/hbase/conf
add_client "hadoop/admin" $KDC_ADMIN_PASS /etc/hadoop/conf/hdfs.keytab $HOSTNAME
chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab
cp /etc/hadoop/conf/hdfs.keytab /etc/hadoop/conf/mapred.keytab
chown mapred:hadoop /etc/hadoop/conf/mapred.keytab
cp /etc/hadoop/conf/hdfs.keytab /etc/hbase/conf/hbase.keytab
chown hbase:hadoop /etc/hbase/conf/hbase.keytab
chmod 640 /etc/hadoop/conf/*.keytab

# Set up local directories for HDFS and MapReduce

umount /dev/xvdb
mkfs.xfs -b size=4096 -f /dev/xvdb
m="/media/ephemeral0"
mkdir -p $m
mount -o defaults,noatime,nodiratime /dev/xvdb $m
mkdir -p $m/dfs/data $m/mapred/local $m/logs $m/zookeeper
chmod -R 0750 $m/dfs/data
chown -R hadoop:hadoop $m/logs
chown -R hdfs:hadoop $m/dfs
chown -R mapred:hadoop $m/mapred
chown -R hbase:hadoop $m/zookeeper
DFS_NAME_DIR="$m/dfs/name"
DFS_DATA_DIR="$m/dfs/data"
MAPRED_LOCAL_DIR="$m/mapred/local"
mkfs.xfs -b size=4096 -f /dev/xvdc
if [ $? -eq 0 ] ; then
  m="/media/ephemeral1"
  mkdir -p $m
  mount -o defaults,noatime,nodiratime /dev/xvdc $m
  mkdir -p $m/dfs/data $m/mapred/local $m/zookeeper
  chmod -R 0750 $m/dfs/data
  chown -R hdfs:hadoop $m/dfs
  chown -R mapred:hadoop $m/mapred
  chown -R hbase:hadoop $m/zookeeper
  DFS_NAME_DIR="$DFS_NAME_DIR,$m/dfs/name"
  DFS_DATA_DIR="$DFS_DATA_DIR,$m/dfs/data"
  MAPRED_LOCAL_DIR="$MAPRED_LOCAL_DIR,$m/mapred/local"
  i=2
  for d in d e f g h i ; do
    mkfs.xfs -b size=4096 -f /dev/xvd${d}
    if [ $? -eq 0 ] ; then
      m="/media/ephemeral${i}"
      mkdir -p $m
      mount -o defaults,noatime,nodiratime /dev/xvd${d} $m
      mkdir -p $m/dfs/data $m/mapred/local
      chmod -R 0750 $m/dfs/data
      chown -R hdfs:hadoop $m/dfs
      chown -R mapred:hadoop $m/mapred
      DFS_DATA_DIR="$DFS_DATA_DIR,$m/dfs/data"
      MAPRED_LOCAL_DIR="$MAPRED_LOCAL_DIR,$m/mapred/local"
      i=$(( i + 1 ))
    else
      break
    fi
  done
fi
DFS_NAMESECONDARY_DIR=`echo $DFS_NAME_DIR | sed -e 's/name/namesecondary/'`

# Set up Hadoop configuration

kinit=`which kinit`

# retarget logs to /media/ephemeral0
rm -rf /var/log/hadoop*
mkdir -p /media/ephemeral0/logs/hadoop
chown root:hadoop /media/ephemeral0/logs/hadoop
chmod -R 01770 /media/ephemeral0/logs
ln -s /media/ephemeral0/logs/hadoop /var/log/hadoop
rm -rf /usr/lib/hadoop/logs
ln -s /var/log/hadoop /usr/lib/hadoop/logs
# make sure permissions are correct in /var/run
chown -R root:hadoop /var/run/hadoop*
chmod 775 /var/run/hadoop*

mkdir -p /etc/hadoop/conf
cp -a /usr/lib/hadoop/conf/* /etc/hadoop/conf
rm -rf /usr/lib/hadoop/conf
ln -s /etc/hadoop/conf /usr/lib/hadoop/conf

####################
# HADOOP ENV
#

cat >> /etc/hadoop/conf/hadoop-env.sh <<EOF
export JAVA_HOME=/usr/java/latest
export HADOOP_OPTS="\$HADOOP_OPTS -Djavax.security.auth.useSubjectCredsOnly=false"
export HADOOP_NAMENODE_USER=hdfs
export HADOOP_SECONDARYNAMENODE_USER=hdfs
export HADOOP_DATANODE_USER=hdfs
export HADOOP_SECURE_DN_USER=hdfs
export HADOOP_JOBTRACKER_USER=mapred
export HADOOP_TASKTRACKER_USER=mapred
export JSVC_HOME=/usr/bin/
EOF

####################
# HADOOP LOG4J
#

cat >> /etc/hadoop/conf/log4j.properties <<EOF
hadoop.root.logger=INFO,console
hadoop.log.dir=.
hadoop.log.file=hadoop.log
hadoop.mapreduce.jobsummary.logger=\${hadoop.root.logger}
hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
hadoop.metrics.log.level=INFO
log4j.rootLogger=\${hadoop.root.logger}, EventCounter
log4j.threshhold=ALL
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
log4j.appender.DRFA.File=\${hadoop.log.dir}/\${hadoop.log.file}
log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
hadoop.tasklog.taskid=null
hadoop.tasklog.iscleanup=false
hadoop.tasklog.noKeepSplits=4
hadoop.tasklog.totalLogFileSize=100
hadoop.tasklog.purgeLogSplits=true
hadoop.tasklog.logsRetainHours=12
log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
log4j.appender.TLA.taskId=\${hadoop.tasklog.taskid}
log4j.appender.TLA.isCleanup=\${hadoop.tasklog.iscleanup}
log4j.appender.TLA.totalLogFileSize=\${hadoop.tasklog.totalLogFileSize}
log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
hadoop.security.logger=INFO,console
hadoop.security.log.file=SecurityAuth.audit
log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender 
log4j.appender.DRFAS.File=\${hadoop.log.dir}/\${hadoop.security.log.file}
log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
log4j.logger.SecurityLogger=\${hadoop.security.logger}
log4j.logger.SecurityLogger.additivity=false
log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
log4j.appender.JSA.File=\${hadoop.log.dir}/\${hadoop.mapreduce.jobsummary.log.file}
log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
log4j.appender.JSA.DatePattern=.yyyy-MM-dd
log4j.logger.org.apache.hadoop.mapred.JobInProgress\$JobSummary=\${hadoop.mapreduce.jobsummary.logger}
log4j.additivity.org.apache.hadoop.mapred.JobInProgress\$JobSummary=false
log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=WARN
log4j.logger.org.apache.hadoop.metrics2=\${hadoop.metrics.log.level}
log4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR
EOF

####################
# CORE SITE

cat > /etc/hadoop/conf/core-site.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
 <name>fs.default.name</name>
 <value>hdfs://$MASTER_HOST:8020</value>
</property>
<property>
 <name>fs.checkpoint.dir</name>
 <value>$DFS_NAMESECONDARY_DIR</value>
</property>

<!-- security settings -->
<property>
 <name>hadoop.security.authentication</name>
 <value>kerberos</value>
</property>
<property>
 <name>hadoop.security.authorization</name>
 <value>true</value>
</property>
<property>
 <name>hadoop.kerberos.kinit.command</name>
 <value>$kinit</value>
</property>

</configuration>
EOF

####################
# HDFS SITE

cat > /etc/hadoop/conf/hdfs-site.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
 <name>fs.default.name</name>
 <value>hdfs://$MASTER_HOST:8020</value>
</property>
<property>
 <name>dfs.name.dir</name>
 <value>$DFS_NAME_DIR</value>
</property>
<property>
 <name>dfs.data.dir</name>
 <value>$DFS_DATA_DIR</value>
</property>
<property>
 <name>dfs.support.append</name>
 <value>true</value>
</property>
<property>
 <name>dfs.datanode.max.xcievers</name>
 <value>16384</value>
</property>
<property>
 <name>dfs.datanode.address</name>
 <value>0.0.0.0:1004</value> <!-- requires secure DN starter -->
</property>
<property>
 <name>dfs.datanode.http.address</name>
 <value>0.0.0.0:1006</value> <!-- requires secure DN starter -->
</property>
<property>
 <name>dfs.http.address</name>
 <value>$MASTER_HOST:8070</value>
</property>
<property>
 <name>dfs.https.address</name>
 <value>$MASTER_HOST:8090</value>
</property>
<property>
 <name>dfs.https.port</name>
 <value>8090</value>
</property>
<property>
 <name>dfs.secondary.http.address</name>
 <value>${SECONDARY_HOST:-0.0.0.0}:0</value>
</property>
<property>
 <name>dfs.secondary.https.address</name>
 <value>${SECONDARY_HOST:-0.0.0.0}:8092</value>
</property>
<property>
 <name>dfs.secondary.https.port</name>
 <value>8092</value>
</property>
<property>
 <name>io.bytes.per.checksum</name>
 <value>4096</value>
</property>

<!-- security settings -->
<property>
 <name>dfs.namenode.user.name</name>
 <value>hdfs</value>
</property>
<property>
 <name>dfs.namenode.keytab.file</name>
 <value>/etc/hadoop/conf/hdfs.keytab</value>
</property>	
<property>
 <name>dfs.namenode.kerberos.principal</name>
 <value>hdfs/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>dfs.namenode.kerberos.https.principal</name>
 <value>host/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>dfs.secondary.namenode.user.name</name>
 <value>hdfs</value>
</property>
<property>
 <name>dfs.secondary.namenode.keytab.file</name>
 <value>/etc/hadoop/conf/hdfs.keytab</value>
</property>	
<property>
 <name>dfs.secondary.namenode.kerberos.principal</name>
 <value>hdfs/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>dfs.secondary.namenode.kerberos.https.principal</name>
 <value>host/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>dfs.datanode.data.dir.perm</name>
 <value>750</value> 
</property>
<property>
 <name>dfs.datanode.keytab.file</name>
 <value>/etc/hadoop/conf/hdfs.keytab</value>
</property>	
<property>
 <name>dfs.datanode.kerberos.principal</name>
 <value>hdfs/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>dfs.datanode.kerberos.https.principal</name>
 <value>host/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>dfs.block.access.token.enable</name>
 <value>true</value>
</property>
<!--<property>
  <name>dfs.block.local-path-access.user</name>
  <value>hbase</value>
</property>-->
</configuration>
EOF

####################
# MAPRED SITE

cat > /etc/hadoop/conf/mapred-site.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
 <name>yarn.resourcemanager.address</name>
 <value>$MASTER_HOST:9030</value>
</property>
<property>
  <name>yarn.resourcemanager.resource-tracker.address</name>
  <value>$MASTER_HOST:8031</value>
</property>
<property>
 <name>mapred.job.tracker.http.address</name>
 <value>0.0.0.0:8040</value>
</property>
<property>
 <name>mapred.task.tracker.http.address</name>
 <value>0.0.0.0:8050</value>
</property>
<property>
 <name>yarn.nodemanager.local-dirs</name>
 <value>$MAPRED_LOCAL_DIR</value>
</property>
<property>
 <name>mapred.tmp.dir</name>
 <value>/tmp/mapred</value>
</property>
<property>
 <name>hadoop.log.dir</name>
 <value>/var/log/hadoop</value>
</property>
<property>
 <name>mapred.system.dir</name>
 <value>/mapred/system</value>
</property>
<property>
 <name>mapred.child.java.opts</name>
 <value>-Xmx512m</value>
</property>
<property>
 <name>mapred.tasktracker.map.tasks.maximum</name>
 <value>4</value>
</property>
<property>
 <name>mapred.tasktracker.reduce.tasks.maximum</name>
 <value>1</value>
</property>
<property>
 <name>mapred.jobtracker.taskScheduler</name>
 <value>org.apache.hadoop.mapred.FairScheduler</value>
</property>
<property>
 <name>mapred.fairscheduler.preemption</name>
 <value>true</value>
</property>
<property>
 <name>mapred.fairscheduler.allocation.file</name>
 <value>/etc/hadoop/conf/fair-scheduler.xml</value>
</property>

<!-- security settings -->
<property>
 <name>yarn.resourcemanager.keytab</name>
 <value>/etc/hadoop/conf/mapred.keytab</value>
</property>	
<property>
 <name>yarn.resourcemanager.principal</name>
 <value>mapred/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>yarn.nodemanager.keytab</name>
 <value>/etc/hadoop/conf/mapred.keytab</value>
</property>	
<property>
 <name>yarn.nodemanager.principal</name>
 <value>mapred/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>yarn.acl.enable</name>
 <value>true</value>
</property>
<property>
 <name>mapreduce.cluster.job-authorization-enabled</name>
 <value>true</value>
</property>
<property>
 <name>mapreduce.job.acl-modify-job</name>
 <value>mapred</value>
</property>
<property>
 <name>mapreduce.job.acl-view-job</name>
 <value>mapred</value>
</property>
<property>
 <name>mapred.task.tracker.task-controller</name>
 <value>org.apache.hadoop.mapred.LinuxTaskController</value>
</property>
<property>
 <name>mapreduce.tasktracker.group</name>
 <value>mapred</value>
</property>

</configuration>

####################
# MAPRED QUEUE ACLS

cat > /etc/hadoop/conf/mapred-queue-acls.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
 <name>mapred.queue.default.acl-submit-job</name>
 <value>*</value>
</property>
<property>
 <name>mapred.queue.default.acl-administer-jobs</name>
 <value>mapred</value>
</property>
</configuration>
EOF

####################
# TASK CONTROLLER

cat > /etc/hadoop/conf/taskcontroller.cfg <<EOF
mapred.local.dir=$MAPRED_LOCAL_DIR
hadoop.log.dir=/var/log/hadoop
mapreduce.tasktracker.group=mapred
banned.users=root
min.user.id=10

EOF

####################
# FAIR SCHEDULER

cat > /etc/hadoop/conf/fair-scheduler.xml <<EOF
<?xml version="1.0"?>
<allocations>
 <!-- default pool -->
 <pool name="default">
  <minMaps>1</minMaps>
  <maxMaps>10</maxMaps>
  <minReduces>1</minReduces>
  <maxReduces>10</maxReduces>
 </pool>
 <!-- per user pools -->
 <pool name="mapred">
  <minMaps>1</minMaps>
  <maxMaps>100</maxMaps>
  <minReduces>1</minReduces>
  <maxReduces>10</maxReduces>
 </pool>
 <pool name="hdfs">
  <minMaps>1</minMaps>
  <maxMaps>100</maxMaps>
  <minReduces>1</minReduces>
  <maxReduces>10</maxReduces>
 </pool>
 <pool name="hbase">
  <minMaps>1</minMaps>
  <maxMaps>100</maxMaps>
  <minReduces>1</minReduces>
  <maxReduces>10</maxReduces>
 </pool>
</allocations>
EOF

# work around odd bug locating task controller config
ln -s /etc/hadoop/conf/taskcontroller.cfg /etc/hadoop

cat >> /etc/hadoop/conf/hadoop-env.sh <<EOF
export HADOOP_NAMENODE_OPTS="-Xmn64m -Xms4g -Xmx4g -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseParNewGC"
export HADOOP_SECONDARYNAMENODE_OPTS=`echo "$HADOOP_NAMENODE_OPTS"
export HADOOP_DATANODE_OPTS="-Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseParNewGC"
EOF

rm -f /usr/lib/hbase/bin/hbase
cp /usr/lib/jprofiler_data/hbase /usr/lib/hbase/bin
chmod 777 /usr/lib/hbase/bin/hbase

sed -i "s/localhost/$HOSTNAME/g" /usr/lib/jprofiler_data/rs/config.xml
sed -i "s/localhost/$HOSTNAME/g" /usr/lib/jprofiler_data/master/config.xml

cat > /etc/hadoop/conf/hadoop-metrics.properties <<EOF
dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
dfs.period=10
dfs.servers=$MASTER_HOST:8649
jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
jvm.period=10
jvm.servers=$MASTER_HOST:8649
mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
mapred.period=10
mapred.servers=$MASTER_HOST:8649
EOF

# Set up HBase configuration

# retarget logging to /media/ephemeral0
rm -rf /var/log/hbase*
mkdir -p /media/ephemeral0/logs/hbase
chown root:hbase /media/ephemeral0/logs/hbase
chmod -R 01770 /media/ephemeral0/logs
ln -s /media/ephemeral0/logs/hbase /var/log/hbase

# make sure permissions are correct in /var/run
chown -R root:hbase /var/run/hbase*
chmod 775 /var/run/hbase*

mkdir -p /etc/hbase/conf
cp -a /usr/lib/hbase/conf/* /etc/hbase/conf
rm -rf /usr/lib/hbase/conf
ln -s /etc/hbase/conf /usr/lib/hbase/conf

####################
# HBASE SITE

cp -a /etc/hadoop/conf/configuration.xsl /etc/hbase/conf
cat > /etc/hbase/conf/hbase-site.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
 <name>hbase.rootdir</name>
 <value>hdfs://$MASTER_HOST:8020/hbase</value>
</property>
<property>
 <name>hbase.cluster.distributed</name>
 <value>true</value>
</property>
<property>
 <name>hbase.master.port</name>
 <value>8100</value>
</property>
<property>
 <name>hbase.master.info.port</name>
 <value>8110</value>
</property>
<property>
 <name>hbase.regionserver.port</name>
 <value>8120</value>
</property>
<property>
 <name>hbase.regionserver.info.port</name>
 <value>8130</value>
</property>
<property>
 <name>hbase.hregion.max.filesize</name>
 <value>1073741824</value>
</property>
<property>
 <name>hbase.regionserver.handler.count</name>
 <value>100</value>
</property>
<property>
 <name>hbase.hregion.memstore.block.multiplier</name>
 <value>3</value>
</property>
<property>
 <name>hbase.hstore.blockingStoreFiles</name>
 <value>15</value>
</property>
<property>
 <name>hbase.zookeeper.quorum</name>
 <value>$MASTER_HOST</value>
</property>
<property>
 <name>zookeeper.session.timeout</name>
 <value>60000</value>
</property>
<property>
 <name>hbase.regionserver.lease.period</name>
 <value>300000</value>
</property>
<property>
 <name>hbase.zookeeper.property.maxClientCnxns</name>
 <value>1000</value>
</property>
<property>
 <name>hbase.zookeeper.property.dataDir</name>
 <value>/media/ephemeral0/zookeeper</value>
</property>
<!-- we need to set this high in case the master node is up for a while
  before any slaves are launched in EC2 test clusters -->
<property>
 <name>hbase.server.versionfile.writeattempts</name>
 <value>100</value>
</property>
<property>
    <name>dfs.client.read.shortcircuit</name>
    <value>false</value>
</property>
<property>
    <name>hbase.regionserver.checksum.verify</name>
    <value>false</value>
 </property>

<!-- security settings -->
<property>
 <name>hadoop.security.authorization</name>
 <value>true</value>
</property>
<property>
 <name>hadoop.security.authentication</name>
 <value>kerberos</value>
</property>
<property>
 <name>hbase.security.authentication</name>
 <value>kerberos</value>
</property>
<property>
 <name>hbase.rpc.engine</name>
 <value>org.apache.hadoop.hbase.ipc.SecureRpcEngine</value>
</property>
<property>
 <name>hbase.master.keytab.file</name>
 <value>/etc/hbase/conf/hbase.keytab</value>
</property>	
<property>
 <name>hbase.master.kerberos.principal</name>
 <value>hbase/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>hbase.master.kerberos.https.principal</name>
 <value>host/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>hbase.regionserver.keytab.file</name>
 <value>/etc/hbase/conf/hbase.keytab</value>
</property>	
<property>
 <name>hbase.regionserver.kerberos.principal</name>
 <value>hbase/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>hbase.regionserver.kerberos.https.principal</name>
 <value>host/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>hbase.rest.keytab.file</name>
 <value>/etc/hbase/conf/hbase.keytab</value>
</property>
<property>
 <name>hbase.rest.kerberos.principal</name>
 <value>hbase/_HOST@HADOOP.LOCALDOMAIN</value>
</property>
<property>
 <name>hbase.zookeeper.property.authProvider.1</name>
 <value>org.apache.zookeeper.server.auth.SASLAuthenticationProvider</value>
</property>
<property>
 <name>hbase.zookeeper.property.jaasLoginRenew</name>
 <value>3600000</value>
</property>
<property>
 <name>hbase.coprocessor.master.classes</name>
 <value>org.apache.hadoop.hbase.security.access.AccessController</value>
</property>
<property>
 <name>hbase.coprocessor.region.classes</name>
 <value>org.apache.hadoop.hbase.security.access.AccessController,org.apache.hadoop.hbase.security.token.TokenProvider</value>
</property>

</configuration>
EOF

####################
# IPC PROTOCOL ACLS

cat > /etc/hbase/conf/hadoop-policy.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
 <name>security.client.protocol.acl</name>
 <value>*</value>
</property>
<property>
 <name>security.admin.protocol.acl</name>
 <value>*</value>
</property>
<property>
 <name>security.masterregion.protocol.acl</name>
 <value>*</value>
</property>
</configuration>
EOF

####################
# JAAS CONF (SERVER)

cat > /etc/hbase/conf/jaas.conf <<EOF
Server {
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  keyTab="/etc/hbase/conf/hbase.keytab"
  storeKey=true
  useTicketCache=false
  principal="zookeeper/$HOSTNAME";
};
Client {
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  useTicketCache=false
  keyTab="/etc/hbase/conf/hbase.keytab"
  principal="hbase/$HOSTNAME";
};
EOF

####################
# JAAS CONF (CLIENT)

cat > /etc/hbase/conf/jaas-client.conf <<EOF
Client {
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=false
  useTicketCache=true
  doNotPrompt=true
  renewTGT=true;
};
EOF
chmod 644 /etc/hbase/conf/jaas-client.conf

####################
# HBASE ENV



cat >> /etc/hbase/conf/hbase-env.sh <<EOF
export JAVA_HOME=/usr/java/latest

export HBASE_MASTER_OPTS="\$HBASE_MASTER_OPTS -Xmn64m -Xms2g -Xmx2g -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseParNewGC -Djava.security.auth.login.config=/etc/hbase/conf/jaas.conf -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/var/log/hbase/hbase-master-gc.log"

export HBASE_ZOOKEEPER_OPTS="\$HBASE_ZOOKEEPER_OPTS -Xmn64m -Xms2g -Xmx2g -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseParNewGC -Djava.security.auth.login.config=/etc/hbase/conf/jaas.conf -Dzookeeper.kerberos.removeHostFromPrincipal=true -Dzookeeper.kerberos.removeRealmFromPrincipal=true"
export HBASE_OPTS="\$HBASE_OPTS -Djava.security.auth.login.config=/etc/hbase/conf/jaas-client.conf"
EOF

if [ "$GC" = "CMS" ]; then
 echo export HBASE_REGIONSERVER_OPTS="\$HBASE_REGIONSERVER_OPTS $MEMORY -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseParNewGC -Djava.security.auth.login.config=/etc/hbase/conf/jaas.conf -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/var/log/hbase/hbase-regionserver-gc.log" >> /etc/hbase/conf/hbase-env.sh
fi

if [ "$GC" = "G1GC" ]; then
 echo export HBASE_REGIONSERVER_OPTS="\$HBASE_REGIONSERVER_OPTS $MEMORY -XX:+UseG1GC -XX:+G1ParallelRSetUpdatingEnabled -XX:+G1ParallelRSetScanningEnabled -XX:NewRatio=1 -Djava.security.auth.login.config=/etc/hbase/conf/jaas.conf -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/var/log/hbase/hbase-regionserver-gc.log" >> /etc/hbase/conf/hbase-env.sh
fi


ln -s /etc/hadoop/conf/core-site.xml /etc/hbase/conf
ln -s /etc/hadoop/conf/hdfs-site.xml /etc/hbase/conf
ln -s /etc/hadoop/conf/mapred-site.xml /etc/hbase/conf

##########
# HBASE LOG4J

cat >> /etc/hbase/conf/log4j.properties <<EOF
# Define some default values that can be overridden by system properties
hbase.root.logger=DEBUG,console
hbase.security.logger=INFO,console
hbase.security.log.file=SecurityAuth.audit
hbase.log.dir=/var/log/hbase
hbase.log.file=hbase.log
log4j.rootLogger=\${hbase.root.logger}
log4j.threshhold=ALL

#
# Daily Rolling File Appender
#
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
log4j.appender.DRFA.File=\${hbase.log.dir}/\${hbase.log.file}

# Rollver at midnight
log4j.appender.DRFA.DatePattern=.yyyy-MM-dd

# 30-day backup
#log4j.appender.DRFA.MaxBackupIndex=30
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout

# Pattern format: Date LogLevel LoggerName LogMessage
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n

# Rolling File Appender properties
hbase.log.maxfilesize=256MB
hbase.log.maxbackupindex=20

# Rolling File Appender
log4j.appender.RFA=org.apache.log4j.RollingFileAppender
log4j.appender.RFA.File=\${hbase.log.dir}/\${hbase.log.file}

log4j.appender.RFA.MaxFileSize=\${hbase.log.maxfilesize}
log4j.appender.RFA.MaxBackupIndex=\${hbase.log.maxbackupindex}

log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n


# Debugging Pattern format
#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n

#
# Security audit appender
#
hbase.security.log.file=SecurityAuth.audit
hbase.security.log.maxfilesize=256MB
hbase.security.log.maxbackupindex=20
log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
log4j.appender.RFAS.File=\${hbase.log.dir}/\${hbase.security.log.file}
log4j.appender.RFAS.MaxFileSize=\${hbase.security.log.maxfilesize}
log4j.appender.RFAS.MaxBackupIndex=\${hbase.security.log.maxbackupindex}
log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
log4j.category.SecurityLogger=\${hbase.security.logger}
log4j.additivity.SecurityLogger=false
#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE

#
# Null Appender
#
log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender

#
# console
# Add "console" to rootlogger above if you want to use this 
#
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n

# Custom Logging levels

log4j.logger.org.apache.zookeeper=INFO
#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG
log4j.logger.org.apache.hadoop.hbase=DEBUG
# Make these two classes INFO-level. Make them DEBUG to see more zk debug.
log4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO
log4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO
#log4j.logger.org.apache.hadoop.dfs=DEBUG
# Set this class to log INFO only otherwise its OTT
# Enable this to get detailed connection error/retry logging.
# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE


# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)
#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG

# Uncomment the below if you want to remove logging of client region caching'
# and scan of .META. messages
# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO
# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO
EOF

cat > /etc/hbase/conf/hadoop-metrics.properties <<EOF
dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
dfs.period=10
dfs.servers=$MASTER_HOST:8649
hbase.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
hbase.period=10
hbase.servers=$MASTER_HOST:8649
jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
jvm.period=10
jvm.servers=$MASTER_HOST:8649
EOF

# Configure Ganglia

sed -i -e "s|\( *mcast_join *=.*\)|#\1|" \
  -e "s|\( \sbind *=.*\)|#\1|" \
  -e "s|\(udp_send_channel {\)|\1\n  host=$MASTER_HOST|" \
  /etc/ganglia/gmond.conf

cat > /etc/httpd/conf.d/ganglia.conf <<EOF
Alias /ganglia /usr/share/ganglia
<Location /ganglia>
  Order deny,allow
  Allow from all
</Location>
EOF

# XXX: Temporary fix for ZOOKEEPER-1437 until it is applied upstream

cd /usr/lib/hbase/lib
rm -f zookeeper*.jar 
wget http://tm-files-west.s3.amazonaws.com/zookeeper/zookeeper-3.4.3-1437.jar
ln -s zookeeper-*.jar zookeeper.jar

# XXX: Patch for HBASE-6010

yum -y install patch
cd /usr/lib/hbase/bin
wget -O - http://tm-files-west.s3.amazonaws.com/hbase/hbase-bin.patch | patch
