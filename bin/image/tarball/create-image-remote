#!/bin/bash

export bin="/media/ephemeral0"
unset DISPLAY
mv /tmp/*.sh /tmp/*.pem /tmp/create-image-remote /media/ephemeral0
if [ -f "$bin"/credentials.sh ] ; then
  source "$bin"/credentials.sh
fi
source "$bin"/env.sh

echo "Remote: JAVA_RPM is $JAVA_RPM"
echo "Remote: HADOOP_VERSION is $HADOOP_VERSION"
echo "Remote: HADOOP_URL is $HADOOP_URL"
echo "Remote: HBASE_VERSION is $HBASE_VERSION"
echo "Remote: HBASE_URL is $HBASE_URL"

####################
# Install Java

echo "Installing Java"
cd /media/ephemeral0
wget -O java.rpm $JAVA_RPM
rpm -Uvh java.rpm
rm -f *.rpm
cat > /etc/profile.d/java.sh <<EOF
export JAVA_HOME=/usr/java/latest
export PATH=\$JAVA_HOME/bin:\$PATH
EOF
chmod 755 /etc/profile.d/java.sh

export JAVA_HOME=/usr/java/latest
export PATH=$JAVA_HOME/bin:$PATH

echo "Enabling EPEL repository"

sed -i -e s/enabled=0/enabled=1/ /etc/yum.repos.d/epel.repo

####################
# Install global deps

echo "Installing RPMs"
yum -y install screen ganglia-gmetad ganglia-gmond ganglia-web httpd php lzo xfsprogs krb5-server krb5-workstation


# Sometimes openssl is not getting installed.  Hence installing the same
yum -y install openssl

#The jsvc has to be installed
if [ "$HADOOP_VERSION" = "2.0.3-alpha" ]; then
	yum -y install jsvc
fi

####################
# Set up user accounts

echo "Configuring user accounts"
groupadd hadoop
useradd hadoop -g hadoop
useradd hdfs
useradd mapred
useradd hbase
# hdfs and hbase user must share primary group hadoop for HDFS shortcutting
usermod -a -G hadoop hdfs
usermod -a -G hadoop hbase
usermod -a -G hadoop mapred
# root user needs to be in hadoop group for correct ulimits on datanodes
usermod -a -G hadoop root
# make sure test users can log in
usermod -s /bin/bash -U hadoop
usermod -s /bin/bash -U hbase
usermod -s /bin/bash -U hdfs
usermod -s /bin/bash -U mapred

####################
# Install Hadoop
echo "Checking for hadoop version"
if [ "$HADOOP_VERSION" = "1.0.4" ]; then
	echo "Installing Hadoop $HADOOP_VERSION."
	cd /usr/lib
	wget -O hadoop.tgz $HADOOP_URL
	tar xzf hadoop.tgz
	rm -f hadoop.tgz
	ln -s hadoop-$HADOOP_VERSION hadoop
	mkdir /var/log/hadoop /var/run/hadoop
	chown hdfs:hadoop /var/log/hadoop /var/run/hadoop
	chmod 775 /var/log/hadoop /var/run/hadoop
	ln -s /var/log/hadoop /usr/lib/hadoop/logs
	mkdir -p /etc/hadoop/conf
	rm -rf /usr/lib/hadoop/conf
	ln -s /etc/hadoop/conf /usr/lib/hadoop/conf
	ln -s /usr/lib/hadoop/hadoop-core-$HADOOP_VERSION.jar /usr/lib/hadoop/hadoop-core.jar
	ln -s /usr/lib/hadoop/hadoop-test-$HADOOP_VERSION.jar /usr/lib/hadoop/hadoop-test.jar
	# fix task controller ownership and permissions
	find /usr/lib/hadoop/ -name task-controller -exec chown root:mapred \{} \;
	find /usr/lib/hadoop/ -name task-controller -exec chmod 4750 \{} \;
cat > /etc/profile.d/hadoop.sh <<EOF
export PATH=\$PATH:/usr/lib/hadoop/bin
EOF
	chmod 755 /etc/profile.d/hadoop.sh
fi

####################
if [ "$HADOOP_VERSION" = "2.0.3-alpha" ]; then
# Install Hadoop-2.0.3

echo "Installing Hadoop $HADOOP_VERSION."
cd /usr/lib
wget -O hadoop.tgz $HADOOP_URL
tar xzf hadoop.tgz
rm -f hadoop.tgz
ln -s hadoop-$HADOOP_VERSION hadoop
mkdir /var/log/hadoop /var/run/hadoop
chown hdfs:hadoop /var/log/hadoop /var/run/hadoop
chmod 775 /var/log/hadoop /var/run/hadoop
ln -s /var/log/hadoop /usr/lib/hadoop/logs
mkdir -p /etc/hadoop/conf
rm -rf /usr/lib/hadoop/conf

echo "common"
ln -s /etc/hadoop/conf /usr/lib/hadoop/etc/hadoop/conf
ln -s /usr/lib/hadoop/share/hadoop/common/hadoop-common-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/common/hadoop-common.jar
ln -s /usr/lib/hadoop/share/hadoop/common/hadoop-common-$HADOOP_VERSION-tests.jar /usr/lib/hadoop/share/hadoop/common/hadoop-common-tests.jar

ln -s /usr/lib/hadoop/share/hadoop/hdfs/hadoop-hdfs-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/hdfs/hadoop-hdfs.jar
ln -s /usr/lib/hadoop/share/hadoop/hdfs/hadoop-hdfs-$HADOOP_VERSION-tests.jar /usr/lib/hadoop/share/hadoop/hdfs/hadoop-hdfs-tests.jar

echo "starting mapreduce"
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client.jar
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common.jar
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core.jar
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs.jar
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins.jar
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-plugins-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-plugins.jar
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient.jar
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-$HADOOP_VERSION-tests.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle.jar
ln -s /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples.jar
echo "starting yarn"
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-api-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-api.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-client-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-client.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-common-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-common.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-common.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-$HADOOP_VERSION-tests.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-alpha.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy.jar
ln -s /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-site-$HADOOP_VERSION.jar /usr/lib/hadoop/share/hadoop/yarn/hadoop-yarn-site.jar
echo "yarn completed"

# Replace the RFA to DRFA based on our log4j.properties
sed -i s/RFA/DRFA/ /usr/lib/hadoop/sbin/hadoop-daemon.sh

# fix task controller ownership and permissions
find /usr/lib/hadoop/ -name task-controller -exec chown root:mapred \{} \;
find /usr/lib/hadoop/ -name task-controller -exec chmod 4750 \{} \;
cat > /etc/profile.d/hadoop.sh <<EOF
export PATH=\$PATH:/usr/lib/hadoop/bin
EOF
chmod 755 /etc/profile.d/hadoop.sh
fi
####################
if [ "$HBASE_VERSION" = "0.94.5" ]; then
# Install HBase

echo "Installing HBase $HBASE_VERSION."
cd /usr/lib
wget -O hbase.tgz $HBASE_URL
tar xzf hbase.tgz
rm -f hbase.tgz
ln -s hbase-$HBASE_VERSION-security hbase
mkdir /var/log/hbase /var/run/hbase
chown hbase:hadoop /var/log/hbase /var/run/hbase
chmod 775 /var/log/hbase /var/run/hbase
ln -s /var/log/hbase /usr/lib/hbase/logs
mkdir -p /etc/hbase/conf
rm -rf /usr/lib/hbase/conf
ln -s /etc/hbase/conf /usr/lib/hbase/conf
# version independent symlinks
ln -s /usr/lib/hbase/hbase-$HBASE_VERSION-security.jar /usr/lib/hbase/hbase.jar
ln -s /usr/lib/hbase/lib/zookeeper-*.jar /usr/lib/hbase/zookeeper.jar
# replace bundled Hadoop jar with a version independent symlink
rm -f /usr/lib/hbase/lib/hadoop-core-*.jar
ln -s /usr/lib/hadoop/hadoop-core.jar /usr/lib/hbase/lib
# replace lib/native with a symlink to Hadoop native libs
rm -rf /usr/lib/hbase/lib/native
ln -s /usr/lib/hadoop/lib/native  /usr/lib/hbase/lib
cat > /etc/profile.d/hbase.sh <<EOF
export PATH=\$PATH:/usr/lib/hbase/bin
EOF
chmod 755 /etc/profile.d/hbase.sh

# add explicitly to root path
cat >> /root/.bashrc <<EOF
export PATH=\$PATH:/usr/lib/hadoop/bin:/usr/lib/hbase/bin
EOF
fi

####################

if [ "$HBASE_VERSION" = "0.97-SNAPSHOT" ]; then
# Install HBase

echo "Installing HBase $HBASE_VERSION."
cd /usr/lib
wget -O hbase.tgz $HBASE_URL
#Currently hardcoding to get it from Ram's bucket
#s3cmd get  s3://Ram_bucket_us-east-1/hbase-0.97-SNAPSHOT.tar.gz hbase-0.97-SNAPSHOT.tar.gz
tar xzf hbase.tgz
rm -f hbase.tgz
ln -s hbase-$HBASE_VERSION hbase
mkdir /var/log/hbase /var/run/hbase
chown hbase:hadoop /var/log/hbase /var/run/hbase
chmod 775 /var/log/hbase /var/run/hbase
ln -s /var/log/hbase /usr/lib/hbase/logs
mkdir -p /etc/hbase/conf
rm -rf /usr/lib/hbase/conf
ln -s /etc/hbase/conf /usr/lib/hbase/conf
# version independent symlinks
ln -s /usr/lib/hbase/lib/hbase-client-$HBASE_VERSION.jar /usr/lib/hbase/lib/hbase-client.jar
ln -s /usr/lib/hbase/lib/hbase-common-$HBASE_VERSION.jar /usr/lib/hbase/lib/hbase-common.jar
ln -s /usr/lib/hbase/lib/hbase-common-$HBASE_VERSION-tests.jar /usr/lib/hbase/lib/hbase-common-tests.jar
ln -s /usr/lib/hbase/lib/hbase-examples-$HBASE_VERSION.jar /usr/lib/hbase/lib/hbase-examples.jar
ln -s /usr/lib/hbase/lib/hbase-hadoop2-compat-$HBASE_VERSION.jar /usr/lib/hbase/lib/hbase-hadoop2-compat.jar
ln -s /usr/lib/hbase/lib/hbase-hadoop-compat-$HBASE_VERSION.jar /usr/lib/hbase/lib/hbase-hadoop-compat.jar
ln -s /usr/lib/hbase/lib/hbase-it-$HBASE_VERSION.jar /usr/lib/hbase/lib/hbase-it.jar
ln -s /usr/lib/hbase/lib/hbase-it-$HBASE_VERSION-tests.jar /usr/lib/hbase/lib/hbase-it-tests.jar
ln -s /usr/lib/hbase/lib/hbase-prefix-tree-$HBASE_VERSION.jar /usr/lib/hbase/lib/hbase-prefix-tree.jar
ln -s /usr/lib/hbase/lib/hbase-protocol-$HBASE_VERSION.jar /usr/lib/hbase/lib/hbase-protocol.jar
ln -s /usr/lib/hbase/lib/hbase-server-$HBASE_VERSION.jar /usr/lib/hbase/lib/hbase-server.jar
ln -s /usr/lib/hbase/lib/hbase-server-$HBASE_VERSION-tests.jar /usr/lib/hbase/lib/hbase-server-tests.jar

ln -s /usr/lib/hbase/lib/zookeeper-*.jar /usr/lib/hbase/lib/zookeeper.jar
# replace bundled Hadoop jar with a version independent symlink
# There is a problem here in the Trunk version.
# the hadoop-annotations-2.0.3-alpha.jar, hadoop-auth-2.0.3-alpha.jar, hadoop-client-2.0.3-alpha.jar
# are not found in the lib folder of hadoop-2.0.3 tar ball. They are found only here in the lib of hbase
# Currently not creating any symbolic ref
#rm -f /usr/lib/hbase/lib/hadoop-*.jar

#ln -s /usr/lib/hadoop/share/hadoop/common/*.jar /usr/lib/hbase/lib
#ln -s /usr/lib/hadoop/share/hadoop/hdfs/*.jar /usr/lib/hbase/lib
#ln -s /usr/lib/hadoop/share/hadoop/mapreduce/*.jar /usr/lib/hbase/lib
#ln -s /usr/lib/hadoop/share/hadoop/yarn/*.jar /usr/lib/hbase/lib
# replace lib/native with a symlink to Hadoop native libs
rm -rf /usr/lib/hbase/lib/native
ln -s /usr/lib/hadoop/lib/native  /usr/lib/hbase/lib
cat > /etc/profile.d/hbase.sh <<EOF
export PATH=\$PATH:/usr/lib/hbase/bin
EOF
chmod 755 /etc/profile.d/hbase.sh

# add explicitly to root path
cat >> /root/.bashrc <<EOF
export PATH=\$PATH:/usr/lib/hadoop/bin:/usr/lib/hbase/bin
EOF
fi

####################
# Configure system

echo "Configuring system"

echo "@hadoop soft nofile 65536" >> /etc/security/limits.conf
echo "@hadoop hard nofile 65536" >> /etc/security/limits.conf
echo "@hadoop soft nproc 65536" >> /etc/security/limits.conf
echo "@hadoop hard nproc 65536" >> /etc/security/limits.conf
echo "fs.file-max = 65536" >> /etc/sysctl.conf
echo "vm.swappiness = 0" >> /etc/sysctl.conf

[ ! -f /etc/hosts ] &&  echo "127.0.0.1 localhost" > /etc/hosts

# helper wrapper for init
cat > /usr/bin/klogin <<EOF
#!/bin/bash
user=\$1
host=\`hostname -f\`
if [ -z "\$user" ] ; then
  user=\`whoami\`
fi
case \$user in
  hdfs|mapred|hadoop)
    kinit -k -t /etc/hadoop/conf/\$user.keytab \$user/\$host && kinit -R
    ;;
  hbase)
    kinit -k -t /etc/hbase/conf/\$user.keytab \$user/\$host && kinit -R
    ;;
  *)
    echo "No keytab for user \$user"
    ;;
esac
EOF
chmod 755 /usr/bin/klogin

yum -y clean all

for i in httpd gmetad gmond kadmin krb5kdc yum-updatesd ; do
  chkconfig --levels 0123456 $i off
done

rm -f /etc/zookeeper/zoo.cfg

mv /root/.ssh/authorized_keys /media/ephemeral0/

yum -y clean all

cd ~root

####################
# Bundle volume

echo "Bundling volume"
ec2-bundle-vol -d /media/ephemeral0 -k /media/ephemeral0/key.pem -c /media/ephemeral0/cert.pem -u $AWS_ACCOUNT_ID -s 3072 -p $IMAGE_VERSION -r x86_64

# now we can move the key back
mv /media/ephemeral0/authorized_keys /root/.ssh/

####################
# Upload bundle

echo "Uploading new bundle $IMAGE_VERSION"
[ -n "$REGION" -a "$REGION" != "us-east-1" ] && LOC="--location $REGION"
ec2-upload-bundle --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY  --bucket $S3_BUCKET --manifest /media/ephemeral0/$IMAGE_VERSION.manifest.xml $LOC

echo "Done"
