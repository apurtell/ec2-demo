1. Configuration

   Create a file bin/credentials.sh and configure it according to your
   account details:

     export AWS_ENDPOINT=us-west-1.ec2.amazonaws.com
     export EC2_URL=http://$AWS_ENDPOINT/
     export AWS_ACCOUNT_ID=8015-...
     export AWS_ACCESS_KEY_ID=AKIA...
     export AWS_SECRET_ACCESS_KEY=UmU3...
     export EC2_PRIVATE_KEY=/home/apurtell/.ec2/pk-tm-master.pem
     export EC2_CERT=/home/apurtell/.ec2/cert-tm-master.pem
     export EC2_ROOT_SSH_KEY=/home/apurtell/.ec2/root-us-west-1-tm-master.pem

     EC2_ROOT_SSH_KEY should be the path in the local file system where the .pem     file of the keypair that is to be used for creating images/launching instan     ces 	
   Configure bin/env.sh

     Set REGION to the region you would like to use.
   
     Set S3_BUCKET to the bucket where you would like to store the AMI image
     files.

     Set S3_ACCOUNT to the account that owns S3_BUCKET. Be sure to only use
     digits (elide the '-').

   Create a file bin/jprofile_config.xml with jprofiler configurations

2. Create an AMI (optional)

   ./bin/create-image [options]

   where [options] can be one or more of:

     -a <arch>    architecture, default x86_64
     -t <type>    instance type, default m1.small

   This will launch an instance that will build an AMI. 

   When the process of image building is complete the remote instance will
   still be running. The last step of the build procedure will print out
   the command you should execute to terminate the build instace.

3. Launch master

   ./bin/launch-master [options]

   where [options] can be one or more of:

     -a <arch>    architecture, default x86_64
     -t <type>    instance type, default m1.xlarge
     -m           enable monitoring, default no
     --ami <id>   AMI ID

   This command will print out the public DNS name of the master instance if
   successfully launched.

4. Launch slaves

   ./bin/launch-slaves [options] <master> <num slaves>

   where [options] can be one or more of:

     -a <arch>    architecture, default x86_64
     -t <type>    instance type, default m1.xlarge
     -m           enable monitoring, default no
     --ami <id>   AMI ID

   where <master> is the public DNS name of the master instance

   where <num slaves> is the number of slaves to launch

   More slaves can be launched at any time by executing bin/launch-slaves
   again.

5. Backup HBase data

   The HBase data can be backed up to S3. We copy data to S3 using distcp command of Hadoop

   ./bin/backup-data <s3bucket> 

   where <s3bucket> is the S3 bucket to which the data will get backed up.

6. Restore HBase data
   
   The HBase data can be backed up to S3. Later for restoring this data we can use this script. This to be called after the launch of master and slaves are over.
   This launches will start HDFS and MR clusters. We copy data from S3 using distcp command of Hadoop

   ./bin/restore-data [options] <s3bucket> 

   where [options] can be one or more of:

      --ami <id>   AMI ID
   
   where <s3bucket> is the S3 bucket where the data is backed up

7. Launch hbase

   Using this the HBase cluster can be launched. Add the memory setting and GC options to be used by the RSs in bin/env.sh (See HBASE_RS_MEM_GC_OPT in env.sh)

   ./bin/launch-hbase [options]
   
   where [options] can be one or more of:

     --ami <id>   AMI ID
     -p / --profile Whether to profile the RS. We use JProfiler for profiling.

8. SSH to the cluster

   Typically you will want to log in to the master:

   ./bin/ssh-cluster <master>

   where <master> is the public DNS name of the master instance

   Note you can use any public DNS name of any instance in the cluster to log
   in to any of them.
